# Deploying-LLM-using-gradio
Deploying LLM using gradio

Deploying LLM using Gradio

1) Install the requirements
2) Import the liabraries 
3) Set up the tokens 
4) set up the model name
5) Load the tokenizer and the model
6) Ask the model a question
7) Now convert the input text above into tokens pass it to the model and then get a output
8) Now get the output 
9) The response will be in number decode it 
10) And then print the output 

Building the API
1) Import the required liabraries 
2) Create the function
3) Create the interface
4) And then queue it. 

Invoke the Gradio API
1) Install the required liabraries 
2) Paste the url
3) Now the data or prompt 
4) Convert them into json
5) pass the url to client
6) Make the prediction
7) And print the result
